---
permalink: /
title: "A Site for All Things Milan Shah"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome to my page!

Some things about me...
======
My name is Milan Shah and I am currently a Ph.D. student at North Carolina State University under the direction of  [Dr. Michela Becchi](https://mbecchi.wordpress.ncsu.edu/). In addition to my work at NCSU, I have been working with Argonne National Laboratory under the supervision of [Dr. Franck Cappello](https://www.anl.gov/profile/franck-cappello), [Dr. Sheng Di](https://www.anl.gov/profile/sheng-di), and [Dr. Xiaodong Yu](https://xiaodong-yu.github.io/). My research interests lie at the intersection of hardware acceleration, AI/ML, compression, and high performance computing. Everything from writing CUDA code to running large-scale simulations to exploring novel AI techniques excite me, so if any of these topics excite you, please reach out!

Specific topics I deal with...
=====
- Compression algorithms on emerging AI chips, including those from:
  - [Cerebras](https://cerebras.ai/product-system/)
  - [SambaNova](https://sambanova.ai/products/datascale)
  - [Graphcore](https://www.graphcore.ai/products/ipu)
  - [Groq](https://groq.com/groqrack/)
- Using lossy compression to reduce the memory footprint of graph neural networks
- GPU-based lossy compression algorithms (branching off the [SZ compressor family](https://szcompressor.org/))
- Integrating compression with tensor network quantum circuit simulators (like [QTensor](https://github.com/danlkv/QTensor))
- Accelerating random forest classification
- AI/ML algorithms in general!

Questions I'd like answers to...
=====
1. How can we make AI/ML more resource-efficient?
2. In what ways can we leverage novel architectures to meet exponentially growing computational needs?
3. What place does compression have as a tool to reduce power/storage/time requirements?

Some additional thoughts...
=====
I encourage you to explore the rest of the site (such as the [Publications page](https://mkshah5.github.io/publications/)) to see more about some means of addressing these questions. You'll find papers we have published, code repositories ready to be forked, and even a few blog posts to make my research space more accessible. Thanks for tuning in!